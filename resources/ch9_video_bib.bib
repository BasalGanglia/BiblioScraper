@inproceedings{Albu2008,
author={Albu, A.B. and Widsten, B. and Tiange Wang and Lan, J. and Mah, J.},
booktitle={Intelligent Vehicles Symposium, 2008 IEEE},
title={{A computer vision-based system for real-time detection of sleep onset in fatigued drivers}},
year={2008},
pages={25-30},
ISSN={\mbox{1931-0587}},
month={June},
abstract={This paper proposes a novel approach for the real-time detection of sleep onset in fatigued drivers. Sleep onset is the most critical consequence of fatigued driving, as shown by statistics of fatigue-related crashes. Therefore, unlike previous related work, we separate the issue of sleep onset from the global analysis of the physiological state of fatigue. This allows us for formulating our approach as an event-detection problem. Real-time performance is achieved by focusing on a single visual cue (i.e. eye-state), and by a custom-designed template-matching algorithm for on-line eye-state detection.},
}

@article{Allen2007,
  title={Photoplethysmography and its application in clinical physiological measurement},
  author={Allen, John},
  journal={Physiological measurement},
  volume={28},
  number={3},
  pages={R1},
  year={2007},
  publisher={IOP Publishing}
}

@inproceedings{Bernacchia2014,
  title={Non contact measurement of heart and respiration rates based on Kinect\texttrademark},
  author={Bernacchia, Natascia and Scalise, Lorenzo and Casacanditella, Luigi and Ercoli, Ilaria and Marchionni, Paolo and Tomasini, Enrico Primo},
  booktitle={Medical Measurements and Applications (MeMeA), 2014 IEEE International Symposium on},
  pages={1--5},
  year={2014},
  organization={IEEE}
}

@article{Ekman1976,
  title={Measuring facial movement},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental psychology and nonverbal behavior},
  volume={1},
  number={1},
  pages={56--75},
  year={1976},
  publisher={Springer}
}

@article{Ekman1991,
  title={Who can catch a liar?},
  author={Ekman, Paul and O'Sullivan, Maureen},
  journal={American psychologist},
  volume={46},
  number={9},
  pages={913},
  year={1991},
  publisher={American Psychological Association}
}

@article{Ekman2003,
  title={Darwin, deception, and facial expression},
  author={Ekman, Paul},
  journal={Annals of the New York Academy of Sciences},
  volume={1000},
  number={1},
  pages={205--221},
  year={2003},
  publisher={Wiley Online Library}
}

@inproceedings{ElKaliouby2004, 
author={El Kaliouby, R. and Robinson, P.}, 
booktitle={Systems, Man and Cybernetics, 2004 IEEE International Conference on}, 
title={{Mind reading machines: Automated inference of cognitive mental states from video}}, 
year={2004}, 
volume={1}, 
pages={682-688 vol.1}, 
abstract={Mind reading encompasses our ability to attribute mental states to others, and is essential for operating in a complex social environment. The goal in building mind reading machines is to enable computer technologies to understand and react to people's emotions and mental states. This paper describes a system, for the automated inference of cognitive mental states from observed facial expressions and head gestures in video. The system is based on a multilevel dynamic Bayesian network classifier which models cognitive mental states as a number of interacting facial and head displays. Experimental results yield an average recognition rate of 87.4% for 6 mental states groups: Agreement, concentrating, disagreement, interested, thinking and unsure. Real time performance, unobtrusiveness and lack of preprocessing make our system particularly suitable for user-independent human computer interaction}, 
ISSN={\mbox{1062-922X}}, 
}

@article{Gao2014,
  title={Single-shot compressed ultrafast photography at one hundred billion frames per second},
  author={Gao, Liang and Liang, Jinyang and Li, Chiye and Wang, Lihong V},
  journal={Nature},
  volume={516},
  pages={74--77},
  year={2014},
  abstract = {The capture of transient scenes at high imaging speed has been long sought by photographers, with early examples being the well known recording in 1878 of a horse in motion and the 1887 photograph of a supersonic bullet. However, not until the late twentieth century were breakthroughs achieved in demonstrating ultrahigh-speed imaging (more than 105 frames per second). In particular, the introduction of electronic imaging sensors based on the charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) technology revolutionized high-speed photography, enabling acquisition rates of up to 10^7 frames per second. Despite these sensors’ widespread impact, further increasing frame rates using CCD or CMOS technology is fundamentally limited by their on-chip storage and electronic readout speed. Here we demonstrate a two-dimensional dynamic imaging technique, compressed ultrafast photography (CUP), which can capture non-repetitive time-evolving events at up to 10^11 frames per second. Compared with existing ultrafast imaging techniques, CUP has the prominent advantage of measuring an x–y–t (x, y, spatial coordinates; t, time) scene with a single camera snapshot, thereby allowing observation of transient events with temporal resolution as tens of picoseconds. Furthermore, akin to traditional photography, CUP is receive-only, and so does not need the specialized active illumination required by other single-shot ultrafast imagers. As a result, CUP can image a variety of luminescent--such as fluorescent or bioluminescent--objects. Using CUP, we visualize four fundamental physical phenomena with single laser shots only: Laser pulse reflection and refraction, photon racing in two media, and faster-than-light propagation of non-information (that is, motion that appears faster than the speed of light but cannot convey information). Given CUP’s capability, we expect it to find widespread applications in both fundamental and applied sciences, including biomedical research.},
}

@article{Gil2010,
  author={E Gil and M Orini and R Bailón and J M Vergara and L Mainardi and P Laguna},
  title={Photoplethysmography pulse rate variability as a surrogate measurement of heart rate variability during non-stationary conditions},
  journal={Physiological Measurement},
  volume={31},
  number={9},
  pages={1271},
  year={2010},
  abstract={In this paper we assessed the possibility of using the pulse rate variability (PRV) extracted from the photoplethysmography signal as an alternative measurement of the HRV signal in non-stationary conditions. The study is based on analysis of the changes observed during a tilt table test in the heart rate modulation of 17 young subjects. First, the classical indices of HRV analysis were compared to the indices from PRV in intervals where stationarity was assumed. Second, the time-varying spectral properties of both signals were compared by time-frequency (TF) and TF coherence analysis. Third, the effect of replacing PRV with HRV in the assessment of the changes of the autonomic modulation of the heart rate was considered. Time-invariant HRV and PRV indices showed no statistically significant differences ( p > 0.05) and high correlation (>0.97). Time-frequency analysis revealed that the TF spectra of both signals were highly correlated (0.99 ± 0.01); the difference between the instantaneous power, in the LF and HF bands, obtained from HRV and PRV was small (<10 −3 s −2 ) and their temporal patterns were highly correlated (0.98 ± 0.04 and 0.95 ± 0.06 in the LF and HF bands, respectively) and TF coherence in the LF and HF bands was high (0.97 ± 0.04 and 0.89 ± 0.08, respectively). Finally, the instantaneous power in the LF band was observed to significantly increase during head-up tilt by both HRV and PRV analysis. These results suggest that although some differences in the time-varying spectral indices extracted from HRV and PRV exist, mainly in the HF band associated with respiration, PRV could be used as a surrogate of HRV during non-stationary conditions, at least during the tilt table test.}
}

@inproceedings{Gokturk2004, 
	author={Gokturk, S.B. and Yalcin, H. and Bamji, C.}, 
	booktitle={Computer Vision and Pattern Recognition Workshop, 2004. CVPRW '04. Conference on}, 
	title={A Time-Of-Flight Depth Sensor - System Description, Issues and Solutions}, 
	year={2004}, 
	pages={35-35}, 
	month={June},
}

@inproceedings{Kang2008,
 author = {Kang, Sin-Hwa and Watt, James H. and Ala, Sasi Kanth},
 title = {Social Copresence in Anonymous Social Interactions Using a Mobile Video Telephone},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 series = {CHI '08},
 year = {2008},
 isbn = {978-1-60558-011-1},
 location = {Florence, Italy},
 pages = {1535--1544},
 numpages = {10},
 acmid = {1357295},
 publisher = {ACM},
 address = {New York, NY, USA},
 abstract={In this paper, we describe research exploring the effect of behavioral and visual realism of avatars on users' social copresence in emotionally engaged conversations conducted via a simulated mobile video telephone. We offer an elaborated definition of Social Copresence to better measure users' engagement with conversational partners in social interactions that do not involve specific tasks or concrete outcomes. We investigate ways to secure mobile telephone users' anonymity while preserving their most important nonverbal affective behaviors. Experimental results with 180 participants using different combinations of static and dynamic, high and low iconic (both video and graphically animated) avatars show increased Social Copresence with dynamic high-iconic (similar to the human communicator) avatars incorporating correct facial expressions, even when these are presented on the small screen of mobile telephones in such a way that individual identities are masked. The results point to an economical combination of behavioral and iconic realism of avatars that produces maximum emotional engagement in anonymous social interactions using mobile video telephones.},
} 

@article{Lee2015,
	author = {Jaehoon Lee and Min Hong and Sungyong Ryu},
    year = {2015},
    title = {Sleep Monitoring System Using Kinect Sensor},
    journal = {International Journal of Distributed Sensor Networks},
    volume = {2015},
    abstract = {Sleep activity is one of crucial factors for determining the quality of human life. However, a traditional sleep monitoring system onerously requires many devices to be attached to human body for achieving sleep related information. In this paper, we proposed and implemented the sleep monitoring system which can detect the sleep movement and posture during sleep using a Microsoft Kinect v2 sensor without any body attached devices. The proposed sleep monitoring system can readily gather the sleep related information that can reveal the sleep patterns of individuals. We expect that the analyzed sleep related data can significantly improve the sleep quality.}
}

@article{Li2015,
  title={Reading Hidden Emotions: Spontaneous Micro-expression Spotting and Recognition},
  author={Li, Xiaobai and Hong, Xiaopeng and Moilanen, Antti and Huang, Xiaohua and Pfister, Tomas and Zhao, Guoying and Pietik{\"a}inen, Matti},
  journal={arXiv preprint arXiv:1511.00423},
  year={2015}
}

@inproceedings{Lyons1998,
address = {Nara, Japan},
author = {Lyons, M. and Akamatsu, S. and Kamachi, M. and Gyoba, J.},
booktitle = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},
isbn = {0-8186-8344-9},
pages = {200--205},
publisher = {IEEE Comput. Soc},
title = {{Coding facial expressions with Gabor wavelets}},
year = {1998}
}

@article{Ma2009,
	title = "Validation of a three-dimensional facial scanning system based on structured light techniques ",
	journal = "Computer Methods and Programs in Biomedicine ",
	volume = "94",
	number = "3",
	pages = "290 - 298",
	year = "2009",
	note = "",
	issn = "0169-2607",
	author = "Lili Ma and Tianmin Xu and Jiuxiang Lin",
	abstract = "The aim of this study was to validate a newly developed three-dimensional (3D) structured light scanning system in recording the facial morphology. The validation was performed in three aspects including accuracy, precision and reliability. The accuracy and precision were investigated using a plaster model with 19 marked landmarks. The accuracy was determined by comparing the coordinates from the 3D images and from the coordinates measure machine (CMM). The precision was quantified through the repeated landmarks location on 3D images. The reliability was investigated in 10 adult volunteers. Each was scanned five times in 3 weeks. The 3D images acquired at different times were compared with each other to measure the reliability. We found that the accuracy was 0.93 mm, the precision was 0.79 mm, the reliability was 0.2 mm. These findings suggested that the structured light scanning system was accurate, precise and reliable to record the facial morphology for both clinic and research purposes. "
}

@incollection{Martinez2016,
  title={Advances, challenges, and opportunities in automatic facial expression recognition},
  author={Martinez, Brais and Valstar, Michel F},
  booktitle={Advances in Face Detection and Facial Image Analysis},
  pages={63--100},
  year={2016},
  publisher={Springer}
}

@article{Ojala1996,
  title={A comparative study of texture measures with classification based on featured distributions},
  author={Ojala, Timo and Pietik{\"a}inen, Matti and Harwood, David},
  journal={Pattern recognition},
  volume={29},
  number={1},
  pages={51--59},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{Pfister2011,
  title={Recognising spontaneous facial micro-expressions},
  author={Pfister, Tomas and Li, Xiaobai and Zhao, Guoying and Pietik{\"a}inen, Matti},
  booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
  pages={1449--1456},
  year={2011},
  organization={IEEE}
}

@article{Polikovsky2009,
  title={Facial micro-expressions recognition using high speed camera and 3D-gradient descriptor},
  author={Polikovsky, Senya and Kameda, Yoshinari and Ohta, Yuichi},
  year={2009},
  publisher={IET}
}

@article{Samal1992,
  title={Automatic recognition and analysis of human faces and facial expressions: A survey},
  author={Samal, Ashok and Iyengar, Prasana A},
  journal={Pattern recognition},
  volume={25},
  number={1},
  pages={65--77},
  year={1992},
  publisher={Elsevier}
}

@article{Scully2012, 
author={Scully, C. and Jinseok Lee and Meyer, J. and Gorbach, A.M. and Granquist-Fraser, D. and Mendelson, Y. and Chon, K.H.}, 
journal={Biomedical Engineering, IEEE Transactions on}, 
title={Physiological Parameter Monitoring from Optical Recordings With a Mobile Phone}, 
year={2012}, 
volume={59}, 
number={2}, 
pages={303-306}, 
abstract={We show that a mobile phone can serve as an accurate monitor for several physiological variables, based on its ability to record and analyze the varying color signals of a fingertip placed in contact with its optical sensor. We confirm the accuracy of measurements of breathing rate, cardiac R-R intervals, and blood oxygen saturation, by comparisons to standard methods for making such measurements (respiration belts, ECGs, and pulse-oximeters, respectively). Measurement of respiratory rate uses a previously reported algorithm developed for use with a pulse-oximeter, based on amplitude and frequency modulation sequences within the light signal. We note that this technology can also be used with recently developed algorithms for detection of atrial fibrillation or blood loss.}, 
ISSN={\mbox{0018-9294}}, 
month={Feb.},
}

@article{Shan2009,
author = {Caifeng Shan and Shaogang Gong and Peter W. McOwan},
title = {{Facial expression recognition based on Local Binary Patterns: A comprehensive study}},
journal = {Image and Vision Computing},
volume = {27},
number = {6},
pages = {803--816},
year = {2009},
issn = {0262-8856},
abstract = {Automatic facial expression analysis is an interesting and challenging problem, and impacts important applications in many areas such as human–computer interaction and data-driven animation. Deriving an effective facial representation from original face images is a vital step for successful facial expression recognition. In this paper, we empirically evaluate facial representation based on statistical local features, Local Binary Patterns, for person-independent facial expression recognition. Different machine learning methods are systematically examined on several databases. Extensive experiments illustrate that \{LBP\} features are effective and efficient for facial expression recognition. We further formulate Boosted-LBP to extract the most discriminant \{LBP\} features, and the best recognition performance is obtained by using Support Vector Machine classifiers with Boosted-LBP features. Moreover, we investigate \{LBP\} features for low-resolution facial expression recognition, which is a critical problem but seldom addressed in the existing work. We observe in our experiments that \{LBP\} features perform stably and robustly over a useful range of low resolutions of face images, and yield promising performance in compressed low-resolution video sequences captured in real-world environments.},
}

@article{Sun2012a,
author = {Sun, Yu and Hu, Sijung and Azorin-Peris, Vicente and Kalawsky, Roy and Greenwald, Stephen},
title = {{Noncontact imaging photoplethysmography to effectively access pulse rate variability}},
journal = {Journal of Biomedical Optics},
volume = {18},
number = {6},
pages = {061205-061205},
abstract = {Abstract. Noncontact imaging photoplethysmography (PPG) can provide physiological assessment at various anatomical locations with no discomfort to the patient. However, most previous imaging PPG (iPPG) systems have been limited by a low sample frequency, which restricts their use clinically, for instance, in the assessment of pulse rate variability (PRV). In the present study, plethysmographic signals are remotely captured via an iPPG system at a rate of 200fps. The physiological parameters (i.e., heart and respiration rate and PRV) derived from the iPPG datasets yield statistically comparable results to those acquired using a contact PPG sensor, the gold standard. More importantly, we present evidence that the negative influence of initial low sample frequency could be compensated via interpolation to improve the time domain resolution. We thereby provide further strong support for the low-cost webcam-based iPPG technique and, importantly, open up a new avenue for effective noncontact assessment of multiple physiological parameters, with potential applications in the evaluation of cardiac autonomic activity and remote sensing of vital physiological signs.},
year = {2012},
isbn = {1083-3668},
}

@article{Sun2012b,
author = {Sun, Yu and Papin, Charlotte and Azorin-Peris, Vicente and Kalawsky, Roy and Greenwald, Stephen and Hu, Sijung},
title = {{Use of ambient light in remote photoplethysmographic systems: Comparison between a high-performance camera and a low-cost webcam}},
journal = {Journal of Biomedical Optics},
volume = {17},
number = {3},
pages = {037005-1-037005-10},
abstract = {Abstract. Imaging photoplethysmography (PPG) is able to capture useful physiological data remotely from a wide range of anatomical locations. Recent imaging PPG studies have concentrated on two broad research directions involving either high-performance cameras and or webcam-based systems. However, little has been reported about the difference between these two techniques, particularly in terms of their performance under illumination with ambient light. We explore these two imaging PPG approaches through the simultaneous measurement of the cardiac pulse acquired from the face of 10 male subjects and the spectral characteristics of ambient light. Measurements are made before and after a period of cycling exercise. The physiological pulse waves extracted from both imaging PPG systems using the smoothed pseudo-Wigner-Ville distribution yield functional characteristics comparable to those acquired using gold standard contact PPG sensors. The influence of ambient light intensity on the physiological information is considered, where results reveal an independent relationship between the ambient light intensity and the normalized plethysmographic signals. This provides further support for imaging PPG as a means for practical noncontact physiological assessment with clear applications in several domains, including telemedicine and homecare.},
year = {2012},
isbn = {1083-3668},
}

@article{Tarassenko2014,
  author={L Tarassenko and M Villarroel and A Guazzi and J Jorge and D A Clifton and C Pugh},
  title={Non-contact video-based vital sign monitoring using ambient light and auto-regressive models},
  journal={Physiological Measurement},
  volume={35},
  number={5},
  pages={807},
  year={2014},
}

@inproceedings{Wang2006, 
author={Qiong Wang and Jingyu Yang and Mingwu Ren and Yujie Zheng}, 
booktitle={Intelligent Control and Automation, 2006. WCICA 2006. The Sixth World Congress on}, 
title={{Driver fatigue detection: A survey}}, 
year={2006}, 
volume={2}, 
pages={8587-8591}
}

@misc{ TrojanCoffeeCam,
   author = {Stafford-Fraser, Quentin},
   title = {The Trojan Room Coffee Pot},
   year = {1995},
   note = {[Online; accessed 19-July-2016]}
 }

@inproceedings{Xiaoming2003, 
author={Xiaoming Liu and Tsuhan Chen}, 
booktitle={Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on}, 
title={{Video-based face recognition using adaptive hidden Markov models}}, 
year={2003}, 
volume={1}, 
pages={I-340-I-345 vol.1}, 
abstract={While traditional face recognition is typically based on still images, face recognition from video sequences has become popular. In this paper, we propose to use adaptive hidden Markov models (HMM) to perform video-based face recognition. During the training process, the statistics of training video sequences of each subject, and the temporal dynamics, are learned by an HMM. During the recognition process, the temporal characteristics of the test video sequence are analyzed over time by the HMM corresponding to each subject. The likelihood scores provided by the HMMs are compared, and the highest score provides the identity of the test video sequence. Furthermore, with unsupervised learning, each HMM is adapted with the test video sequence, which results in better modeling over time. Based on extensive experiments with various databases, we show that the proposed algorithm results in better performance than using majority voting of image-based recognition results.}, 
ISSN={\mbox{1063-6919}}, 
month={June},
}

@article{Yacoob1996, 
author={Yacoob, Y. and Davis, L.S.}, 
journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on}, 
title={Recognizing human facial expressions from long image sequences using optical flow}, 
year={1996}, 
volume={18}, 
number={6}, 
pages={636-642}, 
ISSN={\mbox{0162-8828}}, 
month={June},
abstract={An approach to the analysis and representation of facial dynamics for recognition of facial expressions from image sequences is presented. The algorithms utilize optical flow computation to identify the direction of rigid and nonrigid motions that are caused by human facial expressions. A mid-level symbolic representation motivated by psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, is demonstrated on a large set of image sequences.}
}

@article{Yeasin2006, 
author={Yeasin, M. and Bullot, B. and Sharma, R.}, 
journal={Multimedia, IEEE Transactions on}, 
title={Recognition of facial expressions and measurement of levels of interest from video}, 
year={2006}, 
volume={8}, 
number={3}, 
pages={500-508}, 
abstract={This paper presents a spatio-temporal approach in recognizing six universal facial expressions from visual data and using them to compute levels of interest. The classification approach relies on a two-step strategy on the top of projected facial motion vectors obtained from video sequences of facial expressions. First a linear classification bank was applied on projected optical flow vectors and decisions made by the linear classifiers were coalesced to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete hidden Markov models (HMMs) to learn the underlying model for each facial expression. The performances of the proposed facial expressions recognition were computed using five fold cross-validation on Cohn-Kanade facial expressions database consisting of 488 video sequences that includes 97 subjects. The proposed approach achieved an average recognition rate of 90.9% on Cohn-Kanade facial expressions database. Recognized facial expressions were mapped to levels of interest using the affect space and the intensity of motion around apex frame. Computed level of interest was subjectively analyzed and was found to be consistent with "ground truth" information in most of the cases. To further illustrate the efficacy of the proposed approach, and also to better understand the effects of a number of factors that are detrimental to the facial expression recognition, a number of experiments were conducted. The first empirical analysis was conducted on a database consisting of 108 facial expressions collected from TV broadcasts and labeled by human coders for subsequent analysis. The second experiment (emotion elicitation) was conducted on facial expressions obtained from 21 subjects by showing the subjects six different movies clips chosen in a manner to arouse spontaneous emotional reactions that would produce natural facial expressions.}, 
ISSN={\mbox{1520-9210}}, 
month={June},
}

@article{Zhao2003,
  title={Face recognition: A literature survey},
  author={Zhao, Wenyi and Chellappa, Rama and Phillips, P Jonathon and Rosenfeld, Azriel},
  journal={ACM computing surveys (CSUR)},
  volume={35},
  number={4},
  pages={399--458},
  year={2003},
  publisher={ACM}
}
